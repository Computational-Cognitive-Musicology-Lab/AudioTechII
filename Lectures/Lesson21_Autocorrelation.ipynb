{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (8, 4)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "from scipy.io.wavfile import read\n",
    "import librosa\n",
    "from librosa import feature, frames_to_time, autocorrelate, clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tempo Estimation\n",
    "\n",
    "The beat (or pulse) of a musical piece is a *periodic* sequence of events or impulses. By examining the periodic structure of the impluses, we can try to estimate what one period is (the distance of a beat), which will allow us to guess the tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation\n",
    "\n",
    "We have learned about correlation. Autocorrelation is used to find how similar a signal, or function, is *to itself* at a certain time difference (or shifted position), which is referred to as *lag*.\n",
    "\n",
    "For example, let's say I have the following sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.array([4,1,3,1,4,1.5,2.5,0.2,3.5,1,2,1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a plot to see it better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autocorrelation will compare this original array to every possible shifted version of itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.roll(vals, -1)\n",
    "b = np.roll(vals, -2)\n",
    "c = np.roll(vals, -3)\n",
    "#d = np.roll(vals, -4)\n",
    "\n",
    "plt.stem(vals)\n",
    "(markers, stemlines, baseline) = plt.stem(b)\n",
    "plt.setp(markers, marker='D', markersize=10, markeredgecolor=\"orange\", markeredgewidth=2)\n",
    "plt.setp(stemlines, color='orange')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(vals, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out on Energy values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall from our last lesson that to estimate onsets, the steps are: \n",
    "1. calculate the energy or RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read\n",
    "(fs, x) = read('../audio/80spopDrums.wav')\n",
    "xnorm = x/np.abs(x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,xnorm.size/fs,1/fs)\n",
    "\n",
    "hop_length = 512 # 50% overlap\n",
    "frame_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from librosa import feature, frames_to_time\n",
    "rms = feature.rms(y=xnorm, hop_length=hop_length, frame_length=frame_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = range(len(rms[0]))\n",
    "t = frames_to_time(frames, sr=fs, hop_length=hop_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "plt.plot(time, xnorm, alpha=0.3)\n",
    "plt.plot(t, rms[0], 'g--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. calculate the frame-to-frame difference in energy. This helps highlight moments of increasing energyâ€”likely places where musical onsets (like drum hits) occur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = rms[0][1:] - rms[0][0:-1]\n",
    "#half-wave rectification - use numpy.where to set elements of array when condition is satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We then apply half-wave rectification (i.e., zeroing out negative values), so only energy increases remain, and decreases have been \"thrown out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_hw = np.where(diff < 0, 0, diff)\n",
    "# diff[diff < 0 ] = 0 # alternative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(t[1:], diff_hw);# drop first time index so that arrays match & have proper start/shift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Locate the frames that pass the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arbitrary cutoff (try testing it out)\n",
    "# you may want to try normalizing these difference values!\n",
    "\n",
    "xplines = diff > 0.03\n",
    "xplines[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can visualize what we've done so far by plotting our signal underneath vertical lines representing our \"guesses\" at the onset positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "plt.plot(time, xnorm, alpha=0.3)\n",
    "plt.plot(t, rms[0], 'g--');\n",
    "\n",
    "# use the boolean mask to locate the frame values exceeding the threshold\n",
    "for xc in t[1:][xplines]:\n",
    "    plt.axvline(x=xc, linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `librosa.clicks` to check how we are doing (https://librosa.org/doc/0.8.0/generated/librosa.clicks.html)by passing our time array of x-axis 'lines' (onset guesses) to convert them to pulses (clicks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicks is a librosa function\n",
    "clicks = librosa.clicks(times=t[1:][xplines], sr=fs, hop_length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xnorm.size, clicks.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks.resize(xnorm.shape)\n",
    "combn = clicks + xnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio(combn, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's where autocorrelation comes in...\n",
    "\n",
    "Let's apply autocorrelation to our energy (measured as RMSE) differential, to look for periodicities in the distance between spikes.\n",
    "\n",
    "Whereas `np.roll()` uses a rotating vector (i.e., takes the values from the end and places them at the beginning) in audio signal autocorrelation we don't want to reorganize the signal values, so the method instead is to remove the values that you have shifted over, and replace them with zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = librosa.autocorrelate(diff_hw) # check defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does `librosa.autocorrelate()` work?\n",
    "\n",
    "When you use `librosa.autocorrelate(x)`, it computes the **non-circular autocorrelation** of the signal. Under the hood, it behaves similarly to:\n",
    "\n",
    "`np.correlate(x, x, mode='full')[len(x)-1:]`\n",
    "\n",
    "#### Mathematical Definition\n",
    "The autocorrelation at lag $\\tau$ is defined as:\n",
    "$$\n",
    "R(\\tau) = \\sum_{t=0}^{N - \\tau - 1} x[t] \\cdot x[t + \\tau]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$x[t]$ is the signal value at time $t$\n",
    "\n",
    "$N$ is the total number of samples\n",
    "\n",
    "$\\tau$ is the lag (in samples)\n",
    "\n",
    "This gives us a measure of how similar the signal is to itself when shifted by $\\tau$ steps.\n",
    "\n",
    "Notice this is **exactly** the same as our DFT process, except we are multiplying and summing not with a basis function (sinusoid) but a shifted version of the input itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(ac)\n",
    "plt.title('Autocorrelation of Energy differential')\n",
    "plt.xlabel('Time lag (in frames)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ac[1:].max()\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.where(ac > 0.4) # ignore zero and one indices\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the times between successive spikes (each frame increments by 512 samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsec = 512/fs\n",
    "fsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array(points)\n",
    "t_inc = fsec * points\n",
    "#ignore first two values (zero and one)\n",
    "t_inc[0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the differences between the values\n",
    "ts = t_inc[0][2:]\n",
    "\n",
    "durs = ts[1:] - ts[:-1]\n",
    "durs = np.round(durs, 3) # round to 3 decimal places\n",
    "durs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get the top N most common elements, let's say top 3 for this example\n",
    "num_counts = Counter(durs)\n",
    "top_n = num_counts.most_common(3)\n",
    "\n",
    "print(top_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that .011, .276, and .534 (approximately) recur (in seconds). 60 divided by these values gets us estimated tempo.\n",
    "\n",
    "I.e., \n",
    "\n",
    "The tempo in beats per minute (BPM) is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Tempo (BPM)} = \\frac{60}{\\text{Seconds per Beat}}\n",
    "$$\n",
    "\n",
    "So if the periodicity we detect is approximately 0.534 seconds:\n",
    "\n",
    "$$\n",
    "\\frac{60}{0.534} \\approx 112.36 \\text{ BPM}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempa = 60 / .012\n",
    "tempb = 60 / .267\n",
    "tempc = 60 / .534\n",
    "\n",
    "print( tempa, tempb, tempc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the \"normal\" range of tempi, tempc is the most likely. Check with tempo calculator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Uses of Autocorrelation in Music DSP\n",
    "\n",
    "- **Pitch Detection**: Repeating waveforms (e.g., from sung vowels) can be tracked using autocorrelation to estimate pitch.\n",
    "- **Meter Estimation**: Beyond tempo, we can use autocorrelation hierarchically to estimate larger structures, such as meters. This involves looking for subharmonics or harmonics in the autocorrelation peaks.\n",
    "- **Loop Point Detection**: In audio looping (like in sampling or sound design), autocorrelation can help find natural looping points by identifying repeated signal patterns.\n",
    "- **Periodicity-Based Segmentation**: Some rhythm or style classifiers use autocorrelation to segment time-series signals based on repeating patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meter Estimation\n",
    "With meter, our beats recur in patterns of stronger and weaker beats. This hierarchical structure can be detected by examining **the periodicity of the signal at multiple levels.**\n",
    "\n",
    "#### Hierarchical Autocorrelation for Meter\n",
    "To estimate meter, we can apply autocorrelation not just at the level of the beat (as we did for tempo) but also at sub-beats and higher-order periodicities (e.g., the measure). This involves looking for autocorrelation peaks at different time scales:\n",
    "\n",
    "**Beat level:** The fundamental period we detected earlier with autocorrelation corresponds to the basic tempo of the piece.\n",
    "\n",
    "**Sub-beat level:** To detect subdivisions of the beat (like eighth notes or sixteenth notes), we look for peaks at half or quarter the beat's period.\n",
    "\n",
    "**Measure level:** We can further examine higher-order periodicities corresponding to measures, typically involving periods that are multiple times the beat.\n",
    "\n",
    "For example: after detecting the beat period, we can check for harmonics of that period (i.e., periods that are integer multiples of the beat period) in the autocorrelation result. The presence of significant peaks at these multiple timescales indicates a strong meter structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4216617ca9a926005cf107b1f83ff5112c6a39449bd781bd80f81cda0d11296e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
